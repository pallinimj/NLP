{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model uses GloVe 6B.100d embeddings and 10,000 vocabulary words.  It also uses 30 neurons and 3 different layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import json\n",
    "import os\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path  # for manipulation of file path names\n",
    "\n",
    "import re  # regular expressions\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 9999\n",
    "\n",
    "# To make output stable across runs\n",
    "def reset_graph(seed= RANDOM_SEED):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "REMOVE_STOPWORDS = False  # no stopword removal \n",
    "\n",
    "EVOCABSIZE = 10000  # specify desired size of pre-defined embedding vocabulary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the pre-defined embeddings source        \n",
    "# Define vocabulary size for the language model    \n",
    "# Create a word_to_embedding_dict for GloVe.6B.50d\n",
    "embeddings_directory = 'embeddings/gloVe.6B'\n",
    "filename = 'glove.6B.100d.txt'\n",
    "embeddings_filename = os.path.join(embeddings_directory, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading embeddings from embeddings/gloVe.6B\\glove.6B.100d.txt\n",
      "Embedding loaded from disks.\n"
     ]
    }
   ],
   "source": [
    "def load_embedding_from_disks(embeddings_filename, with_indexes=True):\n",
    "    \"\"\"\n",
    "    Read a embeddings txt file. If `with_indexes=True`, \n",
    "    we return a tuple of two dictionnaries\n",
    "    `(word_to_index_dict, index_to_embedding_array)`, \n",
    "    otherwise we return only a direct \n",
    "    `word_to_embedding_dict` dictionnary mapping \n",
    "    from a string to a numpy array.\n",
    "    \"\"\"\n",
    "    if with_indexes:\n",
    "        word_to_index_dict = dict()\n",
    "        index_to_embedding_array = []\n",
    "  \n",
    "    else:\n",
    "        word_to_embedding_dict = dict()\n",
    "\n",
    "    with open(embeddings_filename, 'r', encoding='utf-8') as embeddings_file:\n",
    "        for (i, line) in enumerate(embeddings_file):\n",
    "\n",
    "            split = line.split(' ')\n",
    "\n",
    "            word = split[0]\n",
    "\n",
    "            representation = split[1:]\n",
    "            representation = np.array(\n",
    "                [float(val) for val in representation]\n",
    "            )\n",
    "\n",
    "            if with_indexes:\n",
    "                word_to_index_dict[word] = i\n",
    "                index_to_embedding_array.append(representation)\n",
    "            else:\n",
    "                word_to_embedding_dict[word] = representation\n",
    "\n",
    "    # Empty representation for unknown words.\n",
    "    _WORD_NOT_FOUND = [0.0] * len(representation)\n",
    "    if with_indexes:\n",
    "        _LAST_INDEX = i + 1\n",
    "        word_to_index_dict = defaultdict(\n",
    "            lambda: _LAST_INDEX, word_to_index_dict)\n",
    "        index_to_embedding_array = np.array(\n",
    "            index_to_embedding_array + [_WORD_NOT_FOUND])\n",
    "        return word_to_index_dict, index_to_embedding_array\n",
    "    else:\n",
    "        word_to_embedding_dict = defaultdict(lambda: _WORD_NOT_FOUND)\n",
    "        return word_to_embedding_dict\n",
    "\n",
    "print('\\nLoading embeddings from', embeddings_filename)\n",
    "word_to_index, index_to_embedding = \\\n",
    "    load_embedding_from_disks(embeddings_filename, with_indexes=True)\n",
    "print(\"Embedding loaded from disks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding is of shape: (400001, 100)\n",
      "This means (number of words, number of dimensions per word)\n",
      "\n",
      "The first words are words that tend occur more often.\n",
      "Note: for unknown words, the representation is an empty vector,\n",
      "and the index is the last one. The dictionnary has a limit:\n",
      "    A word --> Index in embedding --> Representation\n",
      "    worsdfkljsdf --> 400000 --> [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "    the --> 0 --> [-0.038194, -0.24487, 0.72812, -0.39961, 0.083172, 0.043953, -0.39141, 0.3344, -0.57545, 0.087459, 0.28787, -0.06731, 0.30906, -0.26384, -0.13231, -0.20757, 0.33395, -0.33848, -0.31743, -0.48336, 0.1464, -0.37304, 0.34577, 0.052041, 0.44946, -0.46971, 0.02628, -0.54155, -0.15518, -0.14107, -0.039722, 0.28277, 0.14393, 0.23464, -0.31021, 0.086173, 0.20397, 0.52624, 0.17164, -0.082378, -0.71787, -0.41531, 0.20335, -0.12763, 0.41367, 0.55187, 0.57908, -0.33477, -0.36559, -0.54857, -0.062892, 0.26584, 0.30205, 0.99775, -0.80481, -3.0243, 0.01254, -0.36942, 2.2167, 0.72201, -0.24978, 0.92136, 0.034514, 0.46745, 1.1079, -0.19358, -0.074575, 0.23353, -0.052062, -0.22044, 0.057162, -0.15806, -0.30798, -0.41625, 0.37972, 0.15006, -0.53212, -0.2055, -1.2526, 0.071624, 0.70565, 0.49744, -0.42063, 0.26148, -1.538, -0.30223, -0.073438, -0.28312, 0.37104, -0.25217, 0.016215, -0.017099, -0.38984, 0.87424, -0.72569, -0.51058, -0.52028, -0.1459, 0.8278, 0.27062]\n"
     ]
    }
   ],
   "source": [
    "# shows the general structure of the data structures for word embeddings\n",
    "# This code is modified for our purposes in language modeling \n",
    "vocab_size, embedding_dim = index_to_embedding.shape\n",
    "print(\"Embedding is of shape: {}\".format(index_to_embedding.shape))\n",
    "print(\"This means (number of words, number of dimensions per word)\\n\")\n",
    "print(\"The first words are words that tend occur more often.\")\n",
    "\n",
    "print(\"Note: for unknown words, the representation is an empty vector,\\n\"\n",
    "      \"and the index is the last one. The dictionnary has a limit:\")\n",
    "print(\"    {} --> {} --> {}\".format(\"A word\", \"Index in embedding\", \n",
    "      \"Representation\"))\n",
    "word = \"worsdfkljsdf\"  # a word obviously not in the vocabulary\n",
    "idx = word_to_index[word] # index for word obviously not in the vocabulary\n",
    "complete_vocabulary_size = idx \n",
    "embd = list(np.array(index_to_embedding[idx], dtype=int)) # \"int\" compact print\n",
    "print(\"    {} --> {} --> {}\".format(word, idx, embd))\n",
    "word = \"the\"\n",
    "idx = word_to_index[word]\n",
    "embd = list(index_to_embedding[idx])  # \"int\" for compact print only.\n",
    "print(\"    {} --> {} --> {}\".format(word, idx, embd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_typing_test_sentence = 'The quick brown fox jumps over the lazy dog'\n",
    "\n",
    "def default_factory():\n",
    "    return EVOCABSIZE  # last/unknown-word row in limited_index_to_embedding\n",
    "# dictionary has the items() function, returns list of (key, value) tuples\n",
    "limited_word_to_index = defaultdict(default_factory, \\\n",
    "    {k: v for k, v in word_to_index.items() if v < EVOCABSIZE})\n",
    "\n",
    "# Select the first EVOCABSIZE rows to the index_to_embedding\n",
    "limited_index_to_embedding = index_to_embedding[0:EVOCABSIZE,:]\n",
    "# Set the unknown-word row to be all zeros as previously\n",
    "limited_index_to_embedding = np.append(limited_index_to_embedding, \n",
    "    index_to_embedding[index_to_embedding.shape[0] - 1, :].\\\n",
    "        reshape(1,embedding_dim), \n",
    "    axis = 0)\n",
    "\n",
    "# Delete large numpy array to clear some CPU RAM\n",
    "del index_to_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to get file names within a directory\n",
    "def listdir_no_hidden(path):\n",
    "    start_list = os.listdir(path)\n",
    "    end_list = []\n",
    "    for file in start_list:\n",
    "        if (not file.startswith('.')):\n",
    "            end_list.append(file)\n",
    "    return(end_list)\n",
    "\n",
    "# define list of codes to be dropped from document\n",
    "# carriage-returns, line-feeds, tabs\n",
    "codelist = ['\\r', '\\n', '\\t']   \n",
    "\n",
    "# We will not remove stopwords in this exercise because they are\n",
    "# important to keeping sentences intact\n",
    "if REMOVE_STOPWORDS:\n",
    "    print(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "# previous analysis of a list of top terms showed a number of words, along \n",
    "# with contractions and other word strings to drop from further analysis, add\n",
    "# these to the usual English stopwords to be dropped from a document collection\n",
    "    more_stop_words = ['cant','didnt','doesnt','dont','goes','isnt','hes',\\\n",
    "        'shes','thats','theres','theyre','wont','youll','youre','youve', 'br'\\\n",
    "        've', 're', 'vs'] \n",
    "\n",
    "    some_proper_nouns_to_remove = ['dick','ginger','hollywood','jack',\\\n",
    "        'jill','john','karloff','kudrow','orson','peter','tcm','tom',\\\n",
    "        'toni','welles','william','wolheim','nikita']\n",
    "\n",
    "    # start with the initial list and add to it for movie text work \n",
    "    stoplist = nltk.corpus.stopwords.words('english') + more_stop_words +\\\n",
    "        some_proper_nouns_to_remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_parse(string):\n",
    "    # replace non-alphanumeric with space \n",
    "    temp_string = re.sub('[^a-zA-Z]', '  ', string)    \n",
    "    # replace codes with space\n",
    "    for i in range(len(codelist)):\n",
    "        stopstring = ' ' + codelist[i] + '  '\n",
    "        temp_string = re.sub(stopstring, '  ', temp_string)      \n",
    "    # replace single-character words with space\n",
    "    temp_string = re.sub('\\s.\\s', ' ', temp_string)   \n",
    "    # convert uppercase to lowercase\n",
    "    temp_string = temp_string.lower()    \n",
    "    if REMOVE_STOPWORDS:\n",
    "        # replace selected character strings/stop-words with space\n",
    "        for i in range(len(stoplist)):\n",
    "            stopstring = ' ' + str(stoplist[i]) + ' '\n",
    "            temp_string = re.sub(stopstring, ' ', temp_string)        \n",
    "    # replace multiple blank characters with one blank character\n",
    "    temp_string = re.sub('\\s+', ' ', temp_string)    \n",
    "    return(temp_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Directory: movie-reviews-negative\n",
      "500 files found\n",
      "\n",
      "Processing document files under movie-reviews-negative\n",
      "\n",
      "Negative Reviews loaded\n"
     ]
    }
   ],
   "source": [
    "dir_name = 'movie-reviews-negative'\n",
    "    \n",
    "filenames = listdir_no_hidden(path=dir_name)\n",
    "num_files = len(filenames)\n",
    "\n",
    "for i in range(len(filenames)):\n",
    "    file_exists = os.path.isfile(os.path.join(dir_name, filenames[i]))\n",
    "    assert file_exists\n",
    "print('\\nDirectory:',dir_name)    \n",
    "print('%d files found' % len(filenames))\n",
    "\n",
    "# Read data for negative movie reviews\n",
    "# Data will be stored in a list of lists where the each list represents \n",
    "# a document and document is a list of words.\n",
    "# We then break the text into words.\n",
    "\n",
    "def read_data(filename):\n",
    "\n",
    "  with open(filename, encoding='utf-8') as f:\n",
    "    data = tf.compat.as_str(f.read())\n",
    "    data = data.lower()\n",
    "    data = text_parse(data)\n",
    "    data = TreebankWordTokenizer().tokenize(data)  # The Penn Treebank\n",
    "\n",
    "  return data\n",
    "\n",
    "negative_documents = []\n",
    "\n",
    "print('\\nProcessing document files under', dir_name)\n",
    "for i in range(num_files):\n",
    "    ## print(' ', filenames[i])\n",
    "\n",
    "    words = read_data(os.path.join(dir_name, filenames[i]))\n",
    "\n",
    "    negative_documents.append(words)\n",
    "\n",
    "print('\\nNegative Reviews loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Directory: movie-reviews-positive\n",
      "500 files found\n",
      "\n",
      "Processing document files under movie-reviews-positive\n",
      "\n",
      "Positive Reviews loaded\n"
     ]
    }
   ],
   "source": [
    "dir_name = 'movie-reviews-positive'  \n",
    "filenames = listdir_no_hidden(path=dir_name)\n",
    "num_files = len(filenames)\n",
    "\n",
    "for i in range(len(filenames)):\n",
    "    file_exists = os.path.isfile(os.path.join(dir_name, filenames[i]))\n",
    "    assert file_exists\n",
    "print('\\nDirectory:',dir_name)    \n",
    "print('%d files found' % len(filenames))\n",
    "\n",
    "# Read data for positive movie reviews\n",
    "# Data will be stored in a list of lists where the each list \n",
    "# represents a document and document is a list of words.\n",
    "# We then break the text into words.\n",
    "\n",
    "def read_data(filename):\n",
    "\n",
    "  with open(filename, encoding='utf-8') as f:\n",
    "    data = tf.compat.as_str(f.read())\n",
    "    data = data.lower()\n",
    "    data = text_parse(data)\n",
    "    data = TreebankWordTokenizer().tokenize(data)  # The Penn Treebank\n",
    "\n",
    "  return data\n",
    "\n",
    "positive_documents = []\n",
    "\n",
    "print('\\nProcessing document files under', dir_name)\n",
    "for i in range(num_files):\n",
    "    ## print(' ', filenames[i])\n",
    "\n",
    "    words = read_data(os.path.join(dir_name, filenames[i]))\n",
    "\n",
    "    positive_documents.append(words)\n",
    "    \n",
    "print('\\nPositive Reviews loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_review_length: 1052\n",
      "min_review_length: 22\n"
     ]
    }
   ],
   "source": [
    "max_review_length = 0  # initialize\n",
    "for doc in negative_documents:\n",
    "    max_review_length = max(max_review_length, len(doc))    \n",
    "for doc in positive_documents:\n",
    "    max_review_length = max(max_review_length, len(doc)) \n",
    "print('max_review_length:', max_review_length) \n",
    "\n",
    "min_review_length = max_review_length  # initialize\n",
    "for doc in negative_documents:\n",
    "    min_review_length = min(min_review_length, len(doc))    \n",
    "for doc in positive_documents:\n",
    "    min_review_length = min(min_review_length, len(doc)) \n",
    "print('min_review_length:', min_review_length) \n",
    "\n",
    "# construct list of 1000 lists with 40 words in each list\n",
    "from itertools import chain\n",
    "documents = []\n",
    "for doc in negative_documents:\n",
    "    doc_begin = doc[0:20]\n",
    "    doc_end = doc[len(doc) - 20: len(doc)]\n",
    "    documents.append(list(chain(*[doc_begin, doc_end])))    \n",
    "for doc in positive_documents:\n",
    "    doc_begin = doc[0:20]\n",
    "    doc_end = doc[len(doc) - 20: len(doc)]\n",
    "    documents.append(list(chain(*[doc_begin, doc_end])))    \n",
    "\n",
    "# create list of lists of lists for embeddings\n",
    "embeddings = []    \n",
    "for doc in documents:\n",
    "    embedding = []\n",
    "    for word in doc:\n",
    "       embedding.append(limited_index_to_embedding[limited_word_to_index[word]]) \n",
    "    embeddings.append(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------    \n",
    "# Make embeddings a numpy array for use in an RNN \n",
    "# Create training and test sets with Scikit Learn\n",
    "# -----------------------------------------------------\n",
    "embeddings_array = np.array(embeddings)\n",
    "\n",
    "# Define the labels to be used 500 negative (0) and 500 positive (1)\n",
    "thumbs_down_up = np.concatenate((np.zeros((500), dtype = np.int32), \n",
    "                      np.ones((500), dtype = np.int32)), axis = 0)\n",
    "\n",
    "# Scikit Learn for random splitting of the data  \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Random splitting of the data in to training (80%) and test (20%)  \n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(embeddings_array, thumbs_down_up, test_size=0.20, \n",
    "                     random_state = RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------\n",
    "#Build the Model\n",
    "#Next block creates the model\n",
    "#----------------------------------------\n",
    "\n",
    "#model basics\n",
    "reset_graph()\n",
    "\n",
    "n_steps = embeddings_array.shape[1]  # number of words per document \n",
    "n_inputs = embeddings_array.shape[2]  # dimension of  pre-trained embeddings\n",
    "n_neurons = 30  # analyst specified number of neurons\n",
    "n_outputs = 2  # thumbs-down or thumbs-up\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "y = tf.placeholder(tf.int32, [None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-18-73167723f249>:9: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-18-73167723f249>:9: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n"
     ]
    }
   ],
   "source": [
    "layers = [tf.nn.rnn_cell.BasicRNNCell(num_units = n_neurons,\n",
    "                                     activation = tf.nn.relu)\n",
    "         for layer in range(3)]\n",
    "\n",
    "multi_cell_layer = tf.nn.rnn_cell.MultiRNNCell(layers)\n",
    "outputs, states = tf.nn.dynamic_rnn(multi_cell_layer, X, dtype=tf.float32)\n",
    "states_concat = tf.concat(axis=1, values = states)\n",
    "\n",
    "logits = tf.layers.dense(states_concat, n_outputs)\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\n",
    "                                                          logits=logits)\n",
    "loss = tf.reduce_mean(xentropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "correct = tf.nn.in_top_k(logits, y, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch - 0 Train accuracy: 0.53 Test accuracy: 0.53\n",
      "Epoch - 1 Train accuracy: 0.6 Test accuracy: 0.51\n",
      "Epoch - 2 Train accuracy: 0.62 Test accuracy: 0.52\n",
      "Epoch - 3 Train accuracy: 0.68 Test accuracy: 0.54\n",
      "Epoch - 4 Train accuracy: 0.74 Test accuracy: 0.56\n",
      "Epoch - 5 Train accuracy: 0.72 Test accuracy: 0.56\n",
      "Epoch - 6 Train accuracy: 0.74 Test accuracy: 0.575\n",
      "Epoch - 7 Train accuracy: 0.74 Test accuracy: 0.575\n",
      "Epoch - 8 Train accuracy: 0.75 Test accuracy: 0.59\n",
      "Epoch - 9 Train accuracy: 0.74 Test accuracy: 0.62\n",
      "Epoch - 10 Train accuracy: 0.77 Test accuracy: 0.645\n",
      "Epoch - 11 Train accuracy: 0.79 Test accuracy: 0.65\n",
      "Epoch - 12 Train accuracy: 0.83 Test accuracy: 0.645\n",
      "Epoch - 13 Train accuracy: 0.85 Test accuracy: 0.65\n",
      "Epoch - 14 Train accuracy: 0.87 Test accuracy: 0.67\n",
      "Epoch - 15 Train accuracy: 0.9 Test accuracy: 0.66\n",
      "Epoch - 16 Train accuracy: 0.9 Test accuracy: 0.655\n",
      "Epoch - 17 Train accuracy: 0.91 Test accuracy: 0.65\n",
      "Epoch - 18 Train accuracy: 0.93 Test accuracy: 0.675\n",
      "Epoch - 19 Train accuracy: 0.93 Test accuracy: 0.64\n",
      "Epoch - 20 Train accuracy: 0.95 Test accuracy: 0.67\n",
      "Epoch - 21 Train accuracy: 0.95 Test accuracy: 0.66\n",
      "Epoch - 22 Train accuracy: 0.96 Test accuracy: 0.675\n",
      "Epoch - 23 Train accuracy: 0.97 Test accuracy: 0.65\n",
      "Epoch - 24 Train accuracy: 0.97 Test accuracy: 0.66\n",
      "Epoch - 25 Train accuracy: 0.99 Test accuracy: 0.64\n",
      "Epoch - 26 Train accuracy: 0.98 Test accuracy: 0.64\n",
      "Epoch - 27 Train accuracy: 0.99 Test accuracy: 0.63\n",
      "Epoch - 28 Train accuracy: 0.99 Test accuracy: 0.645\n",
      "Epoch - 29 Train accuracy: 1.0 Test accuracy: 0.615\n",
      "Epoch - 30 Train accuracy: 1.0 Test accuracy: 0.62\n",
      "Epoch - 31 Train accuracy: 1.0 Test accuracy: 0.62\n",
      "Epoch - 32 Train accuracy: 0.91 Test accuracy: 0.56\n",
      "Epoch - 33 Train accuracy: 0.89 Test accuracy: 0.645\n",
      "Epoch - 34 Train accuracy: 0.98 Test accuracy: 0.625\n",
      "Epoch - 35 Train accuracy: 0.93 Test accuracy: 0.615\n",
      "Epoch - 36 Train accuracy: 1.0 Test accuracy: 0.61\n",
      "Epoch - 37 Train accuracy: 0.99 Test accuracy: 0.64\n",
      "Epoch - 38 Train accuracy: 1.0 Test accuracy: 0.66\n",
      "Epoch - 39 Train accuracy: 1.0 Test accuracy: 0.635\n",
      "Epoch - 40 Train accuracy: 1.0 Test accuracy: 0.635\n",
      "Epoch - 41 Train accuracy: 1.0 Test accuracy: 0.645\n",
      "Epoch - 42 Train accuracy: 1.0 Test accuracy: 0.64\n",
      "Epoch - 43 Train accuracy: 1.0 Test accuracy: 0.645\n",
      "Epoch - 44 Train accuracy: 1.0 Test accuracy: 0.645\n",
      "Epoch - 45 Train accuracy: 1.0 Test accuracy: 0.635\n",
      "Epoch - 46 Train accuracy: 1.0 Test accuracy: 0.635\n",
      "Epoch - 47 Train accuracy: 1.0 Test accuracy: 0.63\n",
      "Epoch - 48 Train accuracy: 1.0 Test accuracy: 0.63\n",
      "Epoch - 49 Train accuracy: 1.0 Test accuracy: 0.635\n"
     ]
    }
   ],
   "source": [
    "#fit the model\n",
    "n_epochs = 50\n",
    "batch_size = 100\n",
    "\n",
    "train = []\n",
    "test = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(y_train.shape[0] // batch_size):          \n",
    "            X_batch = X_train[iteration*batch_size:(iteration + 1)*batch_size,:]\n",
    "            y_batch = y_train[iteration*batch_size:(iteration + 1)*batch_size]\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_test = accuracy.eval(feed_dict={X: X_test, y: y_test})\n",
    "        train.append(acc_train)\n",
    "        test.append(acc_test)\n",
    "        print('Epoch -', epoch, 'Train accuracy:', acc_train, 'Test accuracy:', acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50]\n"
     ]
    }
   ],
   "source": [
    "xaxis = []\n",
    "for x in range(1,51):\n",
    "    xaxis.append(x)\n",
    "\n",
    "print(xaxis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title(\"Training the model\", fontsize=14)\n",
    "plt.plot(xaxis, train, \"b.\", markersize=10, label=\"Training\")\n",
    "plt.plot(xaxis, test, \"r.\", markersize=10, label=\"Test\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model produced training accuracy of 100% and testing accuracy of 63.5%.  The model is overfitting, as the test accuracy was higher at epoch 22 than it was at epoch 50."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
